{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the standard Python Data Science libraries for data processing and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling as pp   \n",
    "\n",
    "# Import libraries for statistical tests\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statistics\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import anderson\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import kruskal \n",
    "\n",
    "# Import the H2O AutoML libraries\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "# Import label encoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "\n",
    "# Define how many base models will be built for stacking ensemble\n",
    "max_models=201 # 201 is the winning model count\n",
    "\n",
    "from scipy.stats import skew\n",
    "from matplotlib import pyplot\n",
    "from scipy.stats import boxcox\n",
    "from numpy import exp\n",
    "from math import sqrt\n",
    "import shap \n",
    "# For timeseries analysis\n",
    "import fbprophet\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Import libraries for various types of algorithms and metrics\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "import xgboost\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Import a train/test set into H2O\n",
    "# These libraries are needed only when running this notebook on Azure ML cloud\n",
    "# from azureml.core import Workspace, Datastore, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate H2O server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempts to start and/or connect to and H2O instance\n",
    "# max_mem_size - A character string specifying the maximum size, in bytes, of the memory allocation pool to H2O. This value must a multiple of 1024 greater than 2MB. \n",
    "# Append the letter m or M to indicate megabytes, or g or G to indicate gigabytes. \n",
    "# nthreads - Number of threads in the thread pool. This relates very closely to the number of CPUs used. -1 means use all CPUs on the host (Default). A positive integer specifies the number of CPUs directly. \n",
    "# This value is only used when R starts H2O.\n",
    "\n",
    "h2o.init(\n",
    "    nthreads=-1,     # number of threads when launching a new H2O server\n",
    "    max_mem_size=12  # in gigabytes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Train/Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a train set into H2O\n",
    "# Here, we import the training and testing datasets\n",
    "\n",
    "train = h2o.import_file(\"C:\\\\Data_Science\\\\Competitions\\\\MachineHack-2021\\\\train.csv\")\n",
    "test = h2o.import_file(\"C:\\\\Data_Science\\\\Competitions\\\\MachineHack-2021\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the top 10 records in the training dataset\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the top 10 records in the testing dataset\n",
    "\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert H2O frame to Pandas dataframe(This is done so that data operations can be easily done)\n",
    "\n",
    "train_as_df = h2o.as_list(train, use_pandas=True)\n",
    "test_as_df = h2o.as_list(test, use_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Report\n",
    "\n",
    "    The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.ProfileReport(credit_num)\n",
    "profile=pp.ProfileReport(train_as_df, minimal=False, explorative=True)\n",
    "profile\n",
    "profile.to_file(\"C:\\\\Data_Science\\\\Competitions\\\\MachineHack-2021\\\\profile_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sales\n",
    "train_as_df['Sales'].plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_as_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sales\n",
    "fig, axs = plt.subplots(nrows = 2, ncols=2)\n",
    "fig.set_size_inches(15, 7.5)\n",
    "\n",
    "sns.histplot(train_as_df, x=\"Sales\", hue=\"Outlet_ID\", element=\"step\",stat=\"frequency\", ax=axs[0][0])\n",
    "sns.histplot(train_as_df, x=\"Sales\", hue=\"Outlet_Year\", element=\"step\",stat=\"frequency\", ax=axs[0][1])\n",
    "sns.histplot(train_as_df, x=\"Sales\", hue=\"Outlet_Size\", element=\"step\", stat=\"frequency\",ax=axs[1][0])\n",
    "sns.histplot(train_as_df, x=\"Sales\", hue=\"Outlet_Location_Type\", element=\"step\", stat=\"frequency\",ax=axs[1][1])\n",
    "#sns.histplot(train_as_df, x=\"Sales\", hue=\"Item_Type\", element=\"step\", stat=\"density\",ax=axs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Weight\n",
    "train_as_df['Item_W'].plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item Weight\n",
    "fig, axs = plt.subplots(nrows = 2, ncols=2)\n",
    "fig.set_size_inches(15, 7.5)\n",
    "\n",
    "sns.histplot(train_as_df, x=\"Item_W\", hue=\"Outlet_ID\", element=\"step\", stat=\"frequency\",ax=axs[0][0])\n",
    "sns.histplot(train_as_df, x=\"Item_W\", hue=\"Outlet_Year\", element=\"step\", stat=\"frequency\",ax=axs[0][1])\n",
    "sns.histplot(train_as_df, x=\"Item_W\", hue=\"Outlet_Size\", element=\"step\", stat=\"frequency\",ax=axs[1][0])\n",
    "sns.histplot(train_as_df, x=\"Item_W\", hue=\"Outlet_Location_Type\", element=\"step\",stat=\"frequency\", ax=axs[1][1])\n",
    "#sns.histplot(train_as_df, x=\"Item_W\", hue=\"Item_Type\", element=\"step\",stat=\"density\", ax=axs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item MRP\n",
    "train_as_df['Item_MRP'].plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item MRP\n",
    "fig, axs = plt.subplots(nrows = 2, ncols=2)\n",
    "fig.set_size_inches(15, 7.5)\n",
    "\n",
    "sns.histplot(train_as_df, x=\"Item_MRP\", hue=\"Outlet_ID\", element=\"step\", stat=\"density\",ax=axs[0][0])\n",
    "sns.histplot(train_as_df, x=\"Item_MRP\", hue=\"Outlet_Year\", element=\"step\", stat=\"density\",ax=axs[0][1])\n",
    "sns.histplot(train_as_df, x=\"Item_MRP\", hue=\"Outlet_Size\", element=\"step\", stat=\"density\",ax=axs[1][0])\n",
    "sns.histplot(train_as_df, x=\"Item_MRP\", hue=\"Outlet_Location_Type\", element=\"step\",stat=\"density\", ax=axs[1][1])\n",
    "#sns.histplot(train_as_df, x=\"Item_MRP\", hue=\"Item_Type\", element=\"step\",stat=\"density\", ax=axs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_boxplot(ax,size):\n",
    "    lines = ax.get_lines()\n",
    "    categories = ax.get_xticks()\n",
    "    for cat in categories:\n",
    "        # every 4th line at the interval of 6 is median line\n",
    "        # 0 -> p25 1 -> p75 2 -> lower whisker 3 -> upper whisker 4 -> p50 5 -> upper extreme value\n",
    "        y = round(lines[4+cat*6].get_ydata()[0],1) \n",
    "        ax.text(\n",
    "            cat, \n",
    "            y, \n",
    "            f'{y}', \n",
    "            ha='center', \n",
    "            va='center', \n",
    "            fontweight='bold', \n",
    "            size=size,\n",
    "            color='white',\n",
    "            bbox=dict(facecolor='#445A64'))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 7))\n",
    "fig.set_size_inches(15, 7.5)\n",
    "\n",
    "sns.boxplot(x=\"Outlet_Year\",y=\"Sales\",data=train_as_df, ax=ax[0])\n",
    "ax[0].set_title('Year-wise Sales\\n(Trend)', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax[0].set_xlabel('Year', fontsize = 15, fontdict=dict(weight='bold'))\n",
    "ax[0].set_ylabel('Sales', fontsize = 15, fontdict=dict(weight='bold'))\n",
    "annotate_boxplot(ax[0],11)\n",
    "\n",
    "sns.boxplot(x=\"Outlet_Size\",y=\"Sales\",data=train_as_df, ax=ax[1])\n",
    "ax[1].set_title('Sales Distribution by Outlet Size\\n', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax[1].set_xlabel('Outlet Size', fontsize = 15, fontdict=dict(weight='bold'))\n",
    "ax[1].set_ylabel('Sales', fontsize = 15, fontdict=dict(weight='bold'))\n",
    "annotate_boxplot(ax[1],11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 7))\n",
    "fig.set_size_inches(15, 7.5)\n",
    "\n",
    "sns.boxplot(x=\"Outlet_Location_Type\",y=\"Sales\",data=train_as_df, ax=ax[0])\n",
    "ax[0].set_title('Sales Distribution by Outlet Location Type', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax[0].set_xlabel('Outlet Location Type', fontsize = 15, fontdict=dict(weight='bold'))\n",
    "ax[0].set_ylabel('Sales', fontsize = 15, fontdict=dict(weight='bold'))\n",
    "annotate_boxplot(ax[0],11)\n",
    "\n",
    "sns.boxplot(x=\"Outlet_ID\",y=\"Sales\",data=train_as_df, ax=ax[1])\n",
    "ax[1].set_title('Sales Distribution by Outlet_ID\\n', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax[1].set_xlabel('Outlet_ID', fontsize = 15, fontdict=dict(weight='bold'))\n",
    "ax[1].set_ylabel('Sales', fontsize = 15, fontdict=dict(weight='bold'))\n",
    "annotate_boxplot(ax[1],11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries plotting using Seaborn library\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "d = train_as_df\n",
    "sns.lineplot(d['Outlet_Year'], d['Sales'], marker=\"o\") \n",
    "\n",
    "ax.set_title('Combined Sales over the Years', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax.set_xlabel('Outlet Year', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel('Sales Amount', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "plt.tick_params(axis='y', which='major', labelsize=16)\n",
    "plt.tick_params(axis='x', which='major', labelsize=16)\n",
    "plt.grid()\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='large'  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries plotting using Seaborn library\n",
    "# Sales Distribution over the years as per Outlet ID\n",
    "# Sales Distribution by Outlet ID\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "d = train_as_df\n",
    "sns.lineplot(d['Outlet_Year'], d['Sales'], marker=\"o\", hue=d[\"Outlet_ID\"]) \n",
    "\n",
    "ax.set_title('Sales over the Years by Outlets', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax.set_xlabel('Outlet Year', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel('Sales Amount', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "plt.tick_params(axis='y', which='major', labelsize=16)\n",
    "plt.tick_params(axis='x', which='major', labelsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries plotting using Seaborn library\n",
    "# Sales Distribution over the years as per Outlet ID\n",
    "# Sales Distribution by Outlet_Size\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "d = train_as_df\n",
    "sns.lineplot(d['Outlet_Year'], d['Sales'], marker=\"o\", hue=d[\"Outlet_Size\"]) \n",
    "\n",
    "ax.set_title('Sales over the Years by Outlet Size', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax.set_xlabel('Outlet Year', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel('Sales Amount', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "plt.tick_params(axis='y', which='major', labelsize=16)\n",
    "plt.tick_params(axis='x', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries plotting using Seaborn library\n",
    "# Sales Distribution over the years as per Outlet ID\n",
    "# Sales Distribution by Outlet_Location_Type\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "d = train_as_df\n",
    "sns.lineplot(d['Outlet_Year'], d['Sales'], marker=\"o\", hue=d[\"Outlet_Location_Type\"]) \n",
    "\n",
    "ax.set_title('Sales over the Years by Outlet_Location_Type', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax.set_xlabel('Outlet Year', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel('Sales Amount', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "plt.tick_params(axis='y', which='major', labelsize=16)\n",
    "plt.tick_params(axis='x', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries plotting using Seaborn library\n",
    "# Sales Distribution over the years as per Outlet ID\n",
    "# Sales Distribution by Outlet_Size\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "d = train_as_df\n",
    "sns.lineplot(d['Outlet_Year'], d['Sales'], marker=\"o\", hue=d[\"Item_Type\"]) \n",
    "\n",
    "ax.set_title('Sales over the Years by Item Type', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\n",
    "ax.set_xlabel('Outlet Year', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel('Sales Amount', fontsize = 16, fontdict=dict(weight='bold'))\n",
    "plt.tick_params(axis='y', which='major', labelsize=16)\n",
    "plt.tick_params(axis='x', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Modelling - Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timeseries dataframe with Year and Sales fields\n",
    "# We will do time series modelling using Prophet algorithm from Facebook\n",
    "\n",
    "train_as_ts = train_as_df[['Outlet_Year','Sales']].copy() \n",
    "train_as_ts['Outlet_Year'] = pd.to_datetime(train_as_ts['Outlet_Year'], format='%Y') # Since field has only got year value\n",
    "\n",
    "# Prophet requires columns in this format: ds (Date) and y (value)\n",
    "train_as_ts = train_as_ts.rename(columns={'Outlet_Year': 'ds', 'Sales': 'y'})\n",
    "\n",
    "# Build the prophet model and fit on the training data\n",
    "prophet_model = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "prophet_model.fit(train_as_ts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating the prophet models, I set the changepoint prior to 0.15, up from the default value of 0.05. This hyperparameter is used to control how sensitive the trend is to changes, with a higher value being more sensitive and a lower value less sensitive. This value is used to combat one of the most fundamental trade-offs in machine learning: bias vs. variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a future dataframe for 5 years\n",
    "forecast = prophet_model.make_future_dataframe(periods=5, freq='Y')\n",
    "# Make Sales predictions for next 5 years \n",
    "df_forecast = prophet_model.predict(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, under the predictions table, we are only concerned with ds, yhat_lower, yhat_upper, and yhat because these are the variables that will give us the predicted results with respect to the date specified.\n",
    "\n",
    "yhat means the predicted output based on the input fed to the model, yhat_lower, and upper means the upper and lower value that can go based on the predicted output that is, the fluctuations that can happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the sales forecast for 5 years from 2009-2013\n",
    "\n",
    "df_forecast[['ds','yhat','yhat_lower','yhat_upper']].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the output timeseries\n",
    "\n",
    "prophet_model.plot(df_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the trends in the data\n",
    "\n",
    "prophet_model.plot_components(df_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the trends with respect to year and cyclicity in a year. The first graph represents an slightly decreasing trend as we progress through the years and the latter shows a fluctuating trend in the monthly sales. \n",
    "For most months it is steady but towards the end of the year from December to January there is some fluctuation.\n",
    "The fluctuation gains momemtum between January and February."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_as_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality Distribution Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile-Quantile Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q-q plot\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "qqplot(train_as_df['Sales'], line='s')\n",
    "plt.title('Quantile-Quantile Plot')\n",
    "#plt.savefig(output_dir+\"probability-distribution\\\\qq-plot.png\",bbox_inches = 'tight',pad_inches = 0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk Test\n",
    "\n",
    "    The Shapiro-Wilk test evaluates a data sample and quantifies how likely it is that the data was drawn from a Gaussian distribution, named for Samuel Shapiro and Martin Wilk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo1 = \"H0 : Sample was drawn from a Gaussian distribution , Ha : Sample was not drawn from a Gaussian distribution \\n\" \n",
    "\n",
    "# H0 : Sample was drawn from a Gaussian distribution \n",
    "# Ha : Sample was not drawn from a Gaussian distribution\n",
    "    \n",
    "# p <= alpha: reject H0, not normal.\n",
    "# p > alpha: fail to reject H0, normal.\n",
    "\n",
    "stat, p = shapiro(train_as_df['Sales'])\n",
    "title1 = 'Shapiro-Wilk Test of Normality \\n'\n",
    "print(title1)\n",
    "print(hypo1)\n",
    "print('Statistics=%.4f, p-value=%.4f \\n' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    result = 'Sample looks Gaussian (fail to reject H0)'\n",
    "    print(\"Conclusion:\\n\" ,result)\n",
    "else:\n",
    "    result = 'Sample does not look Gaussian (reject Null Hypothesis H0)'\n",
    "    print(\"Conclusion:\\n\" ,result)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling Test\n",
    "\n",
    "    Anderson-Darling Test is a statistical test that can be used to evaluate whether a data sample comes from one of among many known data samples, named for Theodore Anderson and Donald Darling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo2 = \"H0 : Sample was drawn from a Gaussian distribution , Ha : Sample was not drawn from a Gaussian distribution \\n\" \n",
    "\n",
    "# normality test\n",
    "result = anderson(train_as_df['Sales'])\n",
    "title2 = 'Anderson-Darling Test of Normality \\n'\n",
    "print(title2)\n",
    "print(hypo2)\n",
    "print('Statistic: %.4f \\n' % result.statistic)\n",
    "p = 0\n",
    "for i in range(len(result.critical_values)):\n",
    "\tsl, cv = result.significance_level[i], result.critical_values[i]\n",
    "\tif result.statistic < result.critical_values[i]:\n",
    "\t\tprint('Significance Level %.4f: Critical Value %.4f, Data looks normal (fail to reject Null Hypothesis H0) \\n' % (sl, cv))\n",
    "\telse:\n",
    "\t\tprint('Significance Level %.4f: Critical Value %.4f, Data does not look normal (reject Null Hypothesis H0) \\n' % (sl, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical tests prove that Sales data is not normally distributed. We can try some type of transformations like log, exponential, inversions etc.\n",
    "But those conversions are not helping improve the accuracy in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman Rank Correlation\n",
    "\n",
    "Spearman rank correlation coefficient measures the monotonic relation between two variables. Its values range from -1 to +1 and can be interpreted as:\n",
    "\n",
    "    +1: Perfectly monotonically increasing relationship\n",
    "    +0.8: Strong monotonically increasing relationship\n",
    "    +0.2: Weak monotonically increasing relationship\n",
    "    0: Non-monotonic relation\n",
    "    -0.2: Weak monotonically decreasing relationship\n",
    "    -0.8: Strong monotonically decreasing relationship\n",
    "    -1: Perfectly monotonically decreasing relationship\n",
    "    \n",
    "The Spearman rank-order correlation is a statistical procedure that is designed to measure the relationship between two variables on an ordinal scale of measurement.\n",
    "Pearson correlation assumes that the data we are comparing is normally distributed. When that assumption is not true, the correlation value is reflecting the true association. Spearman correlation does not assume that data is from a specific distribution, so it is a non-parametric correlation measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = train_as_df.corr(method=\"spearman\")\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_correlation(df):\n",
    "    r = df.corr(method=\"spearman\")\n",
    "    plt.figure(figsize=(15,7.5))\n",
    "    sns.color_palette(\"pastel\")\n",
    "    heatmap = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.title(\"Spearman Correlation\")\n",
    "    return(r)\n",
    "display_correlation(train_as_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kruskal-Wallis H Test\n",
    "    The Kruskal-Wallis test is a nonparametric version of the one-way analysis of variance test or ANOVA for short. A Kruskal-Wallis test is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups. It is considered to be the non-parametric equivalent of the One-Way ANOVA.\n",
    "\n",
    "    The default assumption or the null hypothesis is that all data samples were drawn from the same distribution. Specifically, that the population medians of all groups are equal. A rejection of the null hypothesis indicates that there is enough evidence to suggest that one or more samples dominate another sample, but the test does not indicate which samples or by how much.\n",
    "\n",
    "    A significant Kruskal–Wallis test indicates that at least one sample stochastically dominates another sample.If the results of a Kruskal-Wallis test are statistically significant, then it’s appropriate to conduct Dunn’s Test to determine exactly which groups are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0: All sample distributions are equal.\n",
    "# Ha: One or more sample distributions are not equal.\n",
    "\n",
    "kruskal = \"The Kruskal-Wallis test is a nonparametric version of the one-way analysis of variance test or ANOVA for short. A Kruskal-Wallis test is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups. It is considered to be the non-parametric equivalent of the One-Way ANOVA. The default assumption or the null hypothesis is that all data samples were drawn from the same distribution. Specifically, that the population medians of all groups are equal. A rejection of the null hypothesis indicates that there is enough evidence to suggest that one or more samples dominate another sample, but the test does not indicate which samples or by how much. A significant Kruskal–Wallis test indicates that at least one sample stochastically dominates another sample.If the results of a Kruskal-Wallis test are statistically significant, then it’s appropriate to conduct Dunn’s Test to determine exactly which groups are different.\\n\"\n",
    "\n",
    "print(kruskal)\n",
    "small_sales = train_as_df[train_as_df['Outlet_Size']=='Small'].Sales\n",
    "medium_sales = train_as_df[train_as_df['Outlet_Size']=='Medium'].Sales\n",
    "high_sales = train_as_df[train_as_df['Outlet_Size']=='High'].Sales\n",
    "\n",
    "# compare samples\n",
    "stat1, p = stats.kruskal(small_sales, medium_sales, high_sales)\n",
    "print('Statistics for Kruskal-Wallis Test is %.3f, p=%.4f \\n' % (stat1, p)) \n",
    "print(\"Conclusion:\\n\")\n",
    "# Interpret\n",
    "alpha = 0.05  # 95% CI\n",
    "if p > alpha:\n",
    "    print('Same distributions: All sample distributions(small_sales, medium_sales & high_sales) are equal(fail to reject H0)')\n",
    "else:\n",
    "    print('Different distributions: One or more sample distributions(small_sales, medium_sales & high_sales) are not equal(reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0: All sample distributions are equal.\n",
    "# Ha: One or more sample distributions are not equal.\n",
    "\n",
    "kruskal = \"The Kruskal-Wallis test is a nonparametric version of the one-way analysis of variance test or ANOVA for short. A Kruskal-Wallis test is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups. It is considered to be the non-parametric equivalent of the One-Way ANOVA. The default assumption or the null hypothesis is that all data samples were drawn from the same distribution. Specifically, that the population medians of all groups are equal. A rejection of the null hypothesis indicates that there is enough evidence to suggest that one or more samples dominate another sample, but the test does not indicate which samples or by how much. A significant Kruskal–Wallis test indicates that at least one sample stochastically dominates another sample.If the results of a Kruskal-Wallis test are statistically significant, then it’s appropriate to conduct Dunn’s Test to determine exactly which groups are different.\\n\"\n",
    "\n",
    "print(kruskal)\n",
    "tier1_sales = train_as_df[train_as_df['Outlet_Location_Type']=='Tier 1'].Sales\n",
    "tier2_sales = train_as_df[train_as_df['Outlet_Location_Type']=='Tier 2'].Sales\n",
    "tier3_sales = train_as_df[train_as_df['Outlet_Location_Type']=='Tier 3'].Sales\n",
    "\n",
    "# compare samples\n",
    "stat2, p = stats.kruskal(tier1_sales, tier2_sales, tier3_sales)\n",
    "print('Statistics for Kruskal-Wallis Test is %.3f, p=%.4f \\n' % (stat2, p)) \n",
    "print(\"Conclusion:\\n\")\n",
    "# Interpret\n",
    "alpha = 0.05  # 95% CI\n",
    "if p > alpha:\n",
    "    print('Same distributions: All sample distributions(tier1_sales, tier2_sales, tier3_sales) are equal(fail to reject H0)')\n",
    "else:\n",
    "    print('Different distributions: One or more sample distributions(tier1_sales, tier2_sales, tier3_sales) are not equal(reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = \"The eta squared, based on the H-statistic, can be used as the measure of the Kruskal-Wallis test effect size. \\\n",
    "It is calculated as follow : eta2[H] = (H - k + 1)/(n - k); where H is the value obtained in the Kruskal-Wallis test; k is the number of groups; n is the total number of observations. \\\n",
    "The eta-squared estimate assumes values from 0 to 1 and multiplied by 100 indicates the percentage of variance in the dependent variable explained by the independent variable. \\\n",
    "The interpretation values commonly in published literature are: \\\n",
    "   0.01 to < 0.06 (small effect), \\\n",
    "   0.06 to < 0.14 (moderate effect)  \\\n",
    "   >= 0.14 (large effect) \\n\"\n",
    "    \n",
    "print(\"Kruskal Wallis Test Effect Size \\n\")\n",
    "print(eta)\n",
    "    \n",
    "rowcnt=len(train_as_df)\n",
    "stat=stat1\n",
    "\n",
    "def epsilon2(h, n):\n",
    "    return h/((n**2 - 1)/(n+1))\n",
    "epsilon = epsilon2(stat, rowcnt)\n",
    "print('Statistics for Kruskal-Wallis Test is %.3f \\n' % epsilon) \n",
    "print(\"Conclusion:\\n\")\n",
    "\n",
    "if(epsilon >= 0.01 and epsilon < 0.06):\n",
    "    print(\"Variance in the Net Sales explained by the NSS groups is SMALL\\n\")\n",
    "elif(epsilon >= 0.06 and epsilon < 0.14):\n",
    "    print(\"Variance in the Net Sales explained by the NSS groups is MODERATE\\n\")\n",
    "elif(epsilon >= 0.14):\n",
    "    print(\"Variance in the Net Sales explained by the NSS groups is LARGE\\n\")\n",
    "else:\n",
    "    print(\"Unexplained variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kruskal Wallis Test Effect Size \\n\")\n",
    "print(eta)\n",
    "    \n",
    "rowcnt=len(train_as_df)\n",
    "stat=stat2\n",
    "\n",
    "def epsilon2(h, n):\n",
    "    return h/((n**2 - 1)/(n+1))\n",
    "epsilon = epsilon2(stat, rowcnt)\n",
    "print('Statistics for Kruskal-Wallis Test is %.3f \\n' % epsilon) \n",
    "print(\"Conclusion:\\n\")\n",
    "\n",
    "if(epsilon >= 0.01 and epsilon < 0.06):\n",
    "    print(\"Variance in the Net Sales explained by the NSS groups is SMALL\\n\")\n",
    "elif(epsilon >= 0.06 and epsilon < 0.14):\n",
    "    print(\"Variance in the Net Sales explained by the NSS groups is MODERATE\\n\")\n",
    "elif(epsilon >= 0.14):\n",
    "    print(\"Variance in the Net Sales explained by the NSS groups is LARGE\\n\")\n",
    "else:\n",
    "    print(\"Unexplained variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Hoc Test : Dunn's Test\n",
    "\n",
    "    The Kruskal-Wallis test does not identify where this stochastic dominance occurs or for how many pairs of groups stochastic dominance obtains. For analyzing the specific sample pairs for stochastic dominance in post hoc testing, Dunn’s test, pairwise Mann-Whitney tests without Bonferroni correction, or the more powerful but less well-known Conover–Iman test are appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we chose to use a Bonferroni correction for the p-values to control the family-wise error rate, \n",
    "# but other potential choices for the p_adjust argument include: sidak holm-sidak simes-hochberg hommel fdr_bh fdr_by fdr_tsbh\n",
    "# !pip install scikit_posthocs\n",
    "\n",
    "data = [small_sales,medium_sales,high_sales]\n",
    "dunn = sp.posthoc_dunn(data, p_adjust = 'bonferroni')\n",
    "\n",
    "# Change the column names\n",
    "dunn.columns =['Small', 'Medium', 'High'] \n",
    "# Change the row indexes\n",
    "dunn.index = ['Small', 'Medium', 'High']\n",
    "\n",
    "#Change the decimal formatting\n",
    "dunn['Small'] = dunn['Small'].map('{:,.4f}'.format)\n",
    "dunn['Medium'] = dunn['Medium'].map('{:,.4f}'.format)\n",
    "dunn['High'] = dunn['High'].map('{:,.4f}'.format)\n",
    "dunn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we chose to use a Bonferroni correction for the p-values to control the family-wise error rate, \n",
    "# but other potential choices for the p_adjust argument include: sidak holm-sidak simes-hochberg hommel fdr_bh fdr_by fdr_tsbh\n",
    "# !pip install scikit_posthocs\n",
    "\n",
    "data = [tier1_sales,tier2_sales,tier3_sales]\n",
    "dunn = sp.posthoc_dunn(data, p_adjust = 'bonferroni')\n",
    "\n",
    "# Change the column names\n",
    "dunn.columns = ['Tier 1', 'Tier 2', 'Tier 3'] \n",
    "# Change the row indexes\n",
    "dunn.index = ['Tier 1', 'Tier 2', 'Tier 3'] \n",
    "\n",
    "#Change the decimal formatting\n",
    "dunn['Tier 1'] = dunn['Tier 1'].map('{:,.4f}'.format)\n",
    "dunn['Tier 2'] = dunn['Tier 2'].map('{:,.4f}'.format)\n",
    "dunn['Tier 3'] = dunn['Tier 3'].map('{:,.4f}'.format)\n",
    "dunn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "From the results of Dunn’s test we can observe the following:\n",
    "\n",
    "Tier 1 sales, Tier 2 sales and Tier 3 sales are statistically significantly different at α = .05\n",
    "\n",
    "Small outlet sales, Medium outlet sales and High outlet sales are statistically significantly different at α = .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Skewness or Normality Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Item_W and Item_MRP features, lets checks their skewness. Skewness is a measure of symmetry in a distribution. Actually, it’s more correct to describe it as a measure of lack of symmetry. \n",
    "# A standard normal distribution is perfectly symmetrical and has zero skew. \n",
    "\n",
    "data = train_as_df['Item_MRP']\n",
    "print( '\\nSkewness for Item_MRP : ', skew(data))\n",
    "# histogram\n",
    "pyplot.hist(data)\n",
    "pyplot.show()\n",
    "\n",
    "data = train_as_df['Item_W']\n",
    "print( '\\nSkewness for Item_W : ', skew(data))\n",
    "# histogram\n",
    "pyplot.hist(data)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle -ve Sales values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find there are sone -ve sales values which is not clear.\n",
    "# Negative sales number might mean that these are losses , however this is not clerly degined in the problem statement\n",
    "# First, we try by dropping the -ve sales records but its leads to reduced RMSE values\n",
    "# Finally, we found that by turning these -ve numbers to positive , we can get a marginally better RMSE \n",
    "\n",
    "cols = 'Sales'\n",
    "negative_sales = train_as_df[train_as_df[cols] < 0]\n",
    "negative_sales\n",
    "#train = train[train[cols] >= 0] \n",
    "# Convert to absolute values of sales\n",
    "train_as_df['Sales'] = train_as_df['Sales'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection in 'Sales'\n",
    "# After handling outliers in Sales, check the skewness and normality again - it should improve\n",
    "\n",
    "data = train_as_df['Sales']\n",
    "print( '\\nSkewness for Sales : ', skew(data))\n",
    "# histogram\n",
    "pyplot.hist(data)\n",
    "pyplot.show()\n",
    "\n",
    "plt.boxplot(data, vert=False)\n",
    "plt.title(\"Detecting outliers using Boxplot\")\n",
    "plt.xlabel('Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR method of outlier detection\n",
    "\n",
    "    Calculate the interquartile range for the data.\n",
    "    Multiply the interquartile range (IQR) by 1.5 (a constant used to discern outliers).\n",
    "    Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier.\n",
    "    Subtract 1.5 x (IQR) from the first quartile. Any number less than this is a suspected outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the outlier datapoints in 'Sales'\n",
    "\n",
    "# finding the 1st quartile\n",
    "q1 = np.quantile(train_as_df['Sales'], 0.25)\n",
    " \n",
    "# finding the 3rd quartile\n",
    "q3 = np.quantile(train_as_df['Sales'], 0.75)\n",
    "med = np.median(train_as_df['Sales'])\n",
    "print(med)\n",
    " \n",
    "# finding the iqr region\n",
    "iqr = q3-q1\n",
    "\n",
    "print('Median', med)\n",
    "\n",
    "# finding upper and lower whiskers\n",
    "upper_bound = q3+(1.5*iqr)\n",
    "lower_bound = q1-(1.5*iqr)\n",
    "print(iqr, upper_bound, lower_bound)\n",
    "\n",
    "outliers = train_as_df[(train_as_df['Sales'] <= lower_bound) | (train_as_df['Sales'] >= upper_bound)].Sales\n",
    "print('The following are the outliers in the boxplot:{}'.format(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Handling by Winsorization\n",
    "\n",
    "Winsorization is a way to minimize the influence of outliers in your data by either:\n",
    "    Assigning the outlier a lower weight\n",
    "    Changing the value so that it is close to other values in the set\n",
    "\n",
    "The data points are modified, not trimmed/removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers by replacing values above/below a certain threhold with the threshold\n",
    "# here, we have taken the lower and upper thresholds to be 1% and 99%\n",
    "# Winsorization: Percentile based flooring and capping\n",
    "removeOutlier = '0' # 0 means don't exclude outliers, this is just a flag for trying with/without outlier handling\n",
    "\n",
    "df=train_as_df\n",
    "col='Sales'\n",
    "for col in df:\n",
    "    #get dtype for column\n",
    "    dt = df[col].dtype \n",
    "    #check if we want to handle outliers?\n",
    "    if removeOutlier == '1':\n",
    "        #check if it is a numbers\n",
    "        if dt == 'int64' or dt == 'float64':\n",
    "            df[col]=df[col].clip(upper = (df[col].quantile(0.99))) \n",
    "            df[col]=df[col].clip(lower = (df[col].quantile(0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new field Item_Group based on Item_Type\n",
    "# Here, we notice that Item-Type values can be grouped into some common categories of data like Drinks, Non Cosummables and Food\n",
    "# Creating these new features help us better train the model in later stage\n",
    "\n",
    "# we create a list of our IF...ELSE conditions for training data\n",
    "conditions = [\n",
    "    (train_as_df['Item_Type'] == 'Hard Drinks') | (train_as_df['Item_Type'] == 'Soft Drinks'),\n",
    "    (train_as_df['Item_Type'] == 'Others') | (train_as_df['Item_Type'] == 'Household') | (train_as_df['Item_Type'] == 'Health and Hygiene'),\n",
    "    (train_as_df['Item_Type'] == 'Baking Goods') | (train_as_df['Item_Type'] == 'Meat') | (train_as_df['Item_Type'] == 'Starchy Foods') | (train_as_df['Item_Type'] == 'Breads') | (train_as_df['Item_Type'] == 'Seafood'),\n",
    "    (train_as_df['Item_Type'] == 'Fruits and Vegetables') | (train_as_df['Item_Type'] == 'Breakfast') | (train_as_df['Item_Type'] == 'Snack Foods') | (train_as_df['Item_Type'] == 'Frozen Foods') | (train_as_df['Item_Type'] == 'Canned') | (train_as_df['Item_Type'] == 'Dairy')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition in train\n",
    "values = ['Drinks', 'Non_Consummables', 'Food', 'Food']\n",
    "\n",
    "# we create a list of our IF...ELSE conditions for testing data\n",
    "conditions_t = [\n",
    "    (test_as_df['Item_Type'] == 'Hard Drinks') | (test_as_df['Item_Type'] == 'Soft Drinks'),\n",
    "    (test_as_df['Item_Type'] == 'Others') | (test_as_df['Item_Type'] == 'Household') | (test_as_df['Item_Type'] == 'Health and Hygiene'),\n",
    "    (test_as_df['Item_Type'] == 'Baking Goods') | (test_as_df['Item_Type'] == 'Meat') | (test_as_df['Item_Type'] == 'Starchy Foods') | (test_as_df['Item_Type'] == 'Breads') | (test_as_df['Item_Type'] == 'Seafood'),\n",
    "    (test_as_df['Item_Type'] == 'Fruits and Vegetables') | (test_as_df['Item_Type'] == 'Breakfast') | (test_as_df['Item_Type'] == 'Snack Foods') | (test_as_df['Item_Type'] == 'Frozen Foods') | (test_as_df['Item_Type'] == 'Canned') | (test_as_df['Item_Type'] == 'Dairy')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition in test\n",
    "values_t = ['Drinks', 'Non_Consummables', 'Food', 'Food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create Item_Group based on the conditions defined above\n",
    "\n",
    "train_as_df['Item_Group'] = np.select(conditions, values)\n",
    "test_as_df['Item_Group'] = np.select(conditions_t, values_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive the Outlet_Age column\n",
    "\n",
    "# In the given dataset, we have a feayure called Outlet_Year but this by itself is not going to be very useful\n",
    "# We know that the age of an outlet can have some impact on the sales, an older more well known outlet might have more sales than a newer one\n",
    "\n",
    "train_as_df['Outlet_Age'] = 2021 - train_as_df['Outlet_Year']\n",
    "train_as_df=train_as_df.drop(['Outlet_Year'], axis = 1)\n",
    "\n",
    "test_as_df['Outlet_Age'] = 2021 - test_as_df['Outlet_Year']\n",
    "test_as_df=test_as_df.drop(['Outlet_Year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we should not get any NA values, still better to check\n",
    "#Check for any missing values - train\n",
    "\n",
    "round((train_as_df.isnull().sum() * 100/ len(train_as_df)),2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for any missing values - test\n",
    "\n",
    "round((test_as_df.isnull().sum() * 100/ len(test_as_df)),2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the item id into 2 columns item code and item number\n",
    "# This step is no longer applied since it does not improve the RMSE\n",
    "\n",
    "#train['Item_Code'] = [x[:3] for x in train['Item_ID']]\n",
    "#train['Item_Number'] = train['Item_ID'].str[-2:]\n",
    "#train = train.drop(['Item_ID'], axis = 1)\n",
    "\n",
    "#test['Item_Code'] = [x[:3] for x in test['Item_ID']]\n",
    "#test['Item_Number'] = test['Item_ID'].str[-2:]\n",
    "#test = test.drop(['Item_ID'], axis = 1)\n",
    "#\n",
    "#Convert Item_Number to character and sppend a prefix\n",
    "#train['Item_Number'] = '__' + train['Item_Number'].astype(str)\n",
    "#test['Item_Number'] = '__' + test['Item_Number'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical to Numeric Conversion\n",
    "Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric. Since we will be applying a regression algorithm, all the features must be numeric in nature.\n",
    "We can do it by converting the existing categorical columns by applying:\n",
    "\n",
    "    1. Label Encoding\n",
    "    Here, each unique category value is assigned an integer value.We convert the labels into a numeric form so as to convert them into the machine-readable form.\n",
    "    \n",
    "    2. One Hot Encoding \n",
    "    For categorical variables where no ordinal relationship exists, the label encoding is not enough.\n",
    "    In fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor   performance or unexpected results (predictions halfway between categories).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One HOT Encoding\n",
    "# Define a function that will take the original dataframe and features to encode as input, 'one-hot encode' the features and then return the dataframe to calling function.\n",
    "\n",
    "def one_hot_encode(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    original_dataframe = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    original_dataframe=original_dataframe.drop([feature_to_encode], axis = 1)\n",
    "    return(original_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "# Define a function that will take the original dataframe and features to encode as input, 'label encode' the features and then return the dataframe to calling function.\n",
    "\n",
    "def label_encode(original_dataframe, feature_to_encode):\n",
    "    # label_encoder object knows how to understand word labels.\n",
    "    label_encoder = preprocessing.LabelEncoder()    \n",
    "    # Encode labels in column 'species'.\n",
    "    original_dataframe[feature_to_encode]= label_encoder.fit_transform(original_dataframe[feature_to_encode])  \n",
    "    return(original_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the training features\n",
    "\n",
    "train_as_df = one_hot_encode(train_as_df, 'Item_Type')\n",
    "train_as_df = label_encode(train_as_df, 'Outlet_Size')\n",
    "train_as_df = label_encode(train_as_df, 'Outlet_Location_Type')\n",
    "train_as_df = one_hot_encode(train_as_df, 'Outlet_ID')\n",
    "#train_as_df = one_hot_encode(train_as_df, 'Item_Code')\n",
    "#train_as_df = one_hot_encode(train_as_df, 'Item_Number')\n",
    "train_as_df = one_hot_encode(train_as_df, 'Item_Group')\n",
    "train_as_df = one_hot_encode(train_as_df, 'Item_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_as_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the testing features\n",
    "\n",
    "test_as_df = one_hot_encode(test_as_df, 'Item_Type')\n",
    "test_as_df = label_encode(test_as_df, 'Outlet_Size')\n",
    "test_as_df = label_encode(test_as_df, 'Outlet_Location_Type')\n",
    "test_as_df = one_hot_encode(test_as_df, 'Outlet_ID')\n",
    "#test_as_df = one_hot_encode(test_as_df, 'Item_Code')\n",
    "#test_as_df = one_hot_encode(test_as_df, 'Item_Number')\n",
    "test_as_df = one_hot_encode(test_as_df, 'Item_Group')\n",
    "test_as_df = one_hot_encode(test_as_df, 'Item_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_as_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the train and test dataframes for the purpose of building Linear Regression model\n",
    "#This is done because we will use the original dataframes for ensemble modelling at a later stage\n",
    "\n",
    "train_as_df_LINREG = train_as_df.copy()\n",
    "test_as_df_LINREG = test_as_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data in 80:20 ratio\n",
    "\n",
    "feature_columns = train_as_df_LINREG.columns.difference( ['Sales'] )\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_as_df_LINREG[feature_columns],\n",
    "                                                  train_as_df_LINREG['Sales'],\n",
    "                                                  test_size=0.20,\n",
    "                                                  random_state=125)\n",
    "print (len( train_X ))\n",
    "print (len (train_y))\n",
    "print (len( test_X))\n",
    "print (len( test_y))\n",
    "print (train_as_df_LINREG.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Regression \n",
    "# Model initialization\n",
    "regression_model = LinearRegression()\n",
    "# Fit the data(train the model)\n",
    "regression_model.fit(train_X, train_y)\n",
    "\n",
    "# Predict train\n",
    "y_predicted = regression_model.predict(train_X)\n",
    "\n",
    "# model evaluation\n",
    "mse = mean_squared_error(train_y, y_predicted)\n",
    "r2 = r2_score(train_y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "#print('Slope:' ,regression_model.coef_)\n",
    "#print('Intercept:', regression_model.intercept_)\n",
    "print('Train\\n') \n",
    "print('Root Mean Squared Error: ', sqrt(mse)) \n",
    "print('R2 Score: ', r2)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Predict test\n",
    "y_predicted = regression_model.predict(test_X)\n",
    "\n",
    "# model evaluation\n",
    "mse = mean_squared_error(test_y, y_predicted)\n",
    "r2 = r2_score(test_y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "#print('Slope:' ,regression_model.coef_)\n",
    "#print('Intercept:', regression_model.intercept_)\n",
    "print('Test\\n') \n",
    "print('Root Mean Squared Error: ', sqrt(mse)) \n",
    "print('R2 Score: ', r2)\n",
    "\n",
    "#Plot actual vs predicted y values\n",
    "y_pred=regression_model.predict(test_X)\n",
    "plt.figure(figsize=(15,7.5))\n",
    "sns.distplot(y_pred,color=\"Blue\",label=\"Predicted\")\n",
    "sns.distplot(test_y,color=\"Orange\",label=\"Actual\")\n",
    "plt.grid(False)\n",
    "\n",
    "#Save the figure in a file\n",
    "#plt.savefig(output_dir+\"regression\\\\actual_predicted.png\",bbox_inches='tight')    \n",
    "plt.show() \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "clf_rf = RandomForestRegressor(n_estimators=100) # Tried with 100, 200 etc.\n",
    "# Fit the data(train the model)\n",
    "clf_rf.fit(train_X, train_y)\n",
    "\n",
    "# Predict train\n",
    "y_predicted = clf_rf.predict(train_X)\n",
    "\n",
    "# model evaluation\n",
    "mse = mean_squared_error(train_y, y_predicted)\n",
    "r2 = r2_score(train_y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "#print('Slope:' ,regression_model.coef_)\n",
    "#print('Intercept:', regression_model.intercept_)\n",
    "print('Train\\n') \n",
    "print('Root Mean Squared Error: ', sqrt(mse)) \n",
    "print('R2 Score: ', r2)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Predict test\n",
    "y_predicted = regression_model.predict(test_X)\n",
    "\n",
    "# model evaluation\n",
    "mse = mean_squared_error(test_y, y_predicted)\n",
    "r2 = r2_score(test_y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "#print('Slope:' ,regression_model.coef_)\n",
    "#print('Intercept:', regression_model.intercept_)\n",
    "print('Test\\n') \n",
    "print('Root Mean Squared Error: ', sqrt(mse)) \n",
    "print('R2 Score: ', r2)\n",
    "\n",
    "#Plot actual vs predicted y values\n",
    "y_pred=regression_model.predict(test_X)\n",
    "plt.figure(figsize=(15,7.5))\n",
    "sns.distplot(y_pred,color=\"Blue\",label=\"Predicted\")\n",
    "sns.distplot(test_y,color=\"Orange\",label=\"Actual\")\n",
    "plt.grid(False)\n",
    "\n",
    "#Save the figure in a file\n",
    "#plt.savefig(output_dir+\"regression\\\\actual_predicted.png\",bbox_inches='tight')    \n",
    "plt.show() \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = xgboost.XGBRegressor() \n",
    "# Fit the data(train the model)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# Predict train\n",
    "y_predicted = model.predict(train_X)\n",
    "\n",
    "# model evaluation\n",
    "mse = mean_squared_error(train_y, y_predicted)\n",
    "r2 = r2_score(train_y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "#print('Slope:' ,regression_model.coef_)\n",
    "#print('Intercept:', regression_model.intercept_)\n",
    "print('Train\\n') \n",
    "print('Root Mean Squared Error: ', sqrt(mse)) \n",
    "print('R2 Score: ', r2)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Predict test\n",
    "y_predicted = regression_model.predict(test_X)\n",
    "\n",
    "# model evaluation\n",
    "mse = mean_squared_error(test_y, y_predicted)\n",
    "r2 = r2_score(test_y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "#print('Slope:' ,regression_model.coef_)\n",
    "#print('Intercept:', regression_model.intercept_)\n",
    "print('Test\\n') \n",
    "print('Root Mean Squared Error: ', sqrt(mse)) \n",
    "print('R2 Score: ', r2)\n",
    "\n",
    "#Plot actual vs predicted y values\n",
    "y_pred=regression_model.predict(test_X)\n",
    "plt.figure(figsize=(15,7.5))\n",
    "sns.distplot(y_pred,color=\"Blue\",label=\"Predicted\")\n",
    "sns.distplot(test_y,color=\"Orange\",label=\"Actual\")\n",
    "plt.grid(False)\n",
    "\n",
    "#Save the figure in a file\n",
    "#plt.savefig(output_dir+\"regression\\\\actual_predicted.png\",bbox_inches='tight')    \n",
    "plt.show() \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The third method to compute feature importance in Xgboost is to use SHAP package. \n",
    "#It is model-agnostic and using the Shapley values from game theory to estimate the how does each feature contribute to the prediction.\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(test_X)\n",
    "shap.summary_plot(shap_values, test_X, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertune XGBoost\n",
    "# To check: reduce max depth to 5, increase estimators to 1500\n",
    "\n",
    "model = xgboost.XGBRegressor()\n",
    "parameters = {'nthread':[4],\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [0.01], \n",
    "              'max_depth': [5],\n",
    "              'min_child_weight': [3],\n",
    "              'subsample': [1],\n",
    "              'colsample_bytree': [1], \n",
    "              'booster' : ['gbtree'],\n",
    "              'n_estimators': [100]} # try with 1500 trees if there's time\n",
    "\n",
    "model = GridSearchCV(model,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data(train the model)\n",
    "model.fit(train_X, train_y)\n",
    "# Predict train\n",
    "y_predicted = model.predict(train_X)\n",
    "\n",
    "# model evaluation\n",
    "mse = mean_squared_error(train_y, y_predicted)\n",
    "r2 = r2_score(train_y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "#print('Slope:' ,regression_model.coef_)\n",
    "#print('Intercept:', regression_model.intercept_)\n",
    "print('Train\\n') \n",
    "print('Root Mean Squared Error: ', sqrt(mse)) \n",
    "print('R2 Score: ', r2)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Predict test\n",
    "y_predicted = regression_model.predict(test_X)\n",
    "\n",
    "# model evaluation\n",
    "mse = mean_squared_error(test_y, y_predicted)\n",
    "r2 = r2_score(test_y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "#print('Slope:' ,regression_model.coef_)\n",
    "#print('Intercept:', regression_model.intercept_)\n",
    "print('Test\\n') \n",
    "print('Root Mean Squared Error: ', sqrt(mse)) \n",
    "print('R2 Score: ', r2)\n",
    "\n",
    "#Plot actual vs predicted y values\n",
    "y_pred=regression_model.predict(test_X)\n",
    "plt.figure(figsize=(15,7.5))\n",
    "sns.distplot(y_pred,color=\"Blue\",label=\"Predicted\")\n",
    "sns.distplot(test_y,color=\"Orange\",label=\"Actual\")\n",
    "plt.grid(False)\n",
    "\n",
    "#Save the figure in a file\n",
    "#plt.savefig(output_dir+\"regression\\\\actual_predicted.png\",bbox_inches='tight')    \n",
    "plt.show() \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ML models give us decent accuracy, but not great RMSE. We will need RMSE of around 1270 to get a rank in the leaderboard. Therefore, we will try ensemble modelling by stacking models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Ensemble Modelling - H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert pandas dataframe back to H2O frame\n",
    "# Before applying H2O automl algorithms we have to convert the pandas dataframe into H2O readable format\n",
    "# We do the data processing in pandas dataframe format because its faster to do so.\n",
    "\n",
    "train = h2o.H2OFrame(train_as_df)\n",
    "test = h2o.H2OFrame(test_as_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify predictors and response variables\n",
    "# First,  identify predictors and response variables. Since we are predicting ‘Sales’ among datapoints so it will be the response variable. \n",
    "# The remaining variables in the dataframe will form the predictor variables.\n",
    "\n",
    "x = train.columns\n",
    "y = \"Sales\"\n",
    "x.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AutoML for certain base models (limited to 1 hour max runtime by default)\n",
    "# Default number of models is 10 and 1 hour is the default runtime.\n",
    "# The ‘max_models’ argument specifies the number of individuals (or “base”) models and does not include any ensemble models that can be trained separately.\n",
    "# However, through multiple iterations, I found that when model count is between 150-200 its gives the best RMSE\n",
    "# Also, we can run this model on a Unix /Windows machine the difference being on a Windows machine the XGBoost model is not available, so we must run it on Ubuntu server\n",
    "\n",
    "aml = H2OAutoML(max_models=max_models, seed=1) #max_runtime_secs, max_models\n",
    "aml.train(x=x, y=y, training_frame=train)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the AutoML leaderboard\n",
    "\n",
    "Next, we will view the AutoML Leaderboard. Since we did not specify a leaderboard_frame in the H2OAutoML.train() method for scoring and ranking the models, the AutoML leaderboard uses cross-validation metrics to rank the models.\n",
    "\n",
    "A default performance metric for each machine learning task (binary classification, multiclass classification, regression) is specified internally and the leaderboard will be sorted by that metric. In the case of linear regression, the default ranking metric is RMSE(Root Mean Square Error). The leader model is stored at aml.leader and the leaderboard is stored at aml.leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we see that as per RMSE ranking the best model is \"StackedEnsemble_Best1000_1_AutoML_4_20211107_64351\"\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The leader model is stored here\n",
    "# To view details about the best model, its performance metrics on cross-validated data\n",
    "\n",
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate predictions on a test set, you can make predictions\n",
    "# directly on the `\"H2OAutoML\"` object or on the leader model\n",
    "# object directly\n",
    "#preds = aml.predict(test)\n",
    "# or\n",
    "preds = aml.leader.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the prediction with the test dataset. Then we can view the Sales prediction of each outlet\n",
    "\n",
    "df = test.cbind(preds)\n",
    "df.head(5)\n",
    "# Slice cols by vector of names\n",
    "res = df[:, [\"predict\"]]\n",
    "res.head(5)\n",
    "#Rename column\n",
    "res.set_names(['Sales']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Prediction Results\n",
    "    Save the results in a .CSV file. This is the submission file that is to be uploaded on the MachineHack website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the file\n",
    "#h2o.export_file(res, path = \"C:\\\\Data_Science\\\\Competitions\\\\MachineHack-2021\\\\my_submission.csv\", force = True)\n",
    "\n",
    "# Convert to Pandas dataframe\n",
    "# Save as .CSV file\n",
    "res_as_df = h2o.as_list(res, use_pandas=True)\n",
    "res_as_df.to_csv('C:\\\\Data_Science\\\\Competitions\\\\MachineHack-2021\\\\my_submissionFile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model\n",
    "\n",
    "There are two ways to save the leader model -- binary format and MOJO format. If you're taking your leader model to production, \n",
    "then we'd suggest the MOJO format since it's optimized for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.save_model(aml.leader, path = \"C:\\\\Data_Science\\\\Competitions\\\\MachineHack-2021\\\\h20_model_bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.download_mojo(path = \"C:\\\\Data_Science\\\\Competitions\\\\MachineHack-2021\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Exploration\n",
    "\n",
    "    To understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model. The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run. This is often the top performing model on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "se = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n",
    "# Get the Stacked Ensemble metalearner model\n",
    "metalearner = h2o.get_model(se.metalearner()['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the variable importance of the metalearner (combiner) algorithm in the ensemble. This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metalearner is actually the standardized coefficient magnitudes of the GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearner.coef_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also plot the base learner contributions to the ensemble.\n",
    "\n",
    "%matplotlib inline\n",
    "metalearner.std_coef_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
